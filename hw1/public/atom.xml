<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title>Raplis&#x27;s~</title>
    <subtitle>course for IDS721</subtitle>
    <link rel="self" type="application/atom+xml" href="http://18.218.69.233/atom.xml"/>
    <link rel="alternate" type="text/html" href="http://18.218.69.233/"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2021-08-28T14:13:14.674+00:00</updated>
    <id>http://18.218.69.233/atom.xml</id>
    <entry xml:lang="en">
        <title>Huggingface&#x2F;datasets</title>
        <published>2021-08-28T14:13:14.674+00:00</published>
        <updated>2021-08-28T14:13:14.674+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="http://18.218.69.233/opensource/huggingface-datasets/"/>
        <id>http://18.218.69.233/opensource/huggingface-datasets/</id>
        
        <content type="html" xml:base="http://18.218.69.233/opensource/huggingface-datasets/">&lt;p&gt;ðŸ¤— Datasets is a lightweight library providing two main features:&lt;&#x2F;p&gt;
&lt;p&gt;one-line dataloaders for many public datasets: &lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;one liners to download and pre-process any of the number of datasets major public datasets (in 467 languages and dialects!) provided on the HuggingFace Datasets Hub. With a simple command like &lt;code&gt;squad_dataset = load_dataset(&amp;quot;squad&amp;quot;)&lt;&#x2F;code&gt;, get any of these datasets ready to use in a dataloader for training&#x2F;evaluating a ML model (Numpy&#x2F;Pandas&#x2F;PyTorch&#x2F;TensorFlow&#x2F;JAX),&lt;&#x2F;li&gt;
&lt;li&gt;efficient data pre-processing: simple, fast and reproducible data pre-processing for the above public datasets as well as your own local datasets in CSV&#x2F;JSON&#x2F;text. With simple commands like &lt;code&gt;tokenized_dataset = dataset.map(tokenize_example)&lt;&#x2F;code&gt;, efficiently prepare the dataset for inspection and ML model evaluation and training.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>N-LTP: A Open-source Neural Chinese Language Technology Platform with Pretrained Models</title>
        <published>2021-08-28T11:00:37.434+00:00</published>
        <updated>2021-08-28T11:00:37.434+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="http://18.218.69.233/publications/n-ltp-a-open-source-neural-chinese-language-technology-platform-with-pretrained-models/"/>
        <id>http://18.218.69.233/publications/n-ltp-a-open-source-neural-chinese-language-technology-platform-with-pretrained-models/</id>
        
        <content type="html" xml:base="http://18.218.69.233/publications/n-ltp-a-open-source-neural-chinese-language-technology-platform-with-pretrained-models/">&lt;p&gt;An open-source neural language technology platform supporting six fundamental Chinese NLP tasks: &lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;lexical analysis (Chinese word segmentation, part-of-speech tagging, and named entity recognition)&lt;&#x2F;li&gt;
&lt;li&gt;syntactic parsing (dependency parsing)&lt;&#x2F;li&gt;
&lt;li&gt;semantic parsing (semantic dependency parsing and semantic role labeling). &lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Unlike the existing state-of-the-art toolkits, such as Stanza, that adopt an independent model for each task, N-LTP adopts the multi-task framework by using a shared pre-trained model, which has the advantage of capturing the shared knowledge across relevant Chinese tasks. &lt;&#x2F;p&gt;
&lt;p&gt;In addition, knowledge distillation where the single-task model teaches the multi-task model is further introduced to encourage the multi-task model to surpass its single-task teacher.&lt;&#x2F;p&gt;
&lt;p&gt;Finally, we provide a collection of easy-to-use APIs and a visualization tool to make users easier to use and view the processing results directly. To the best of our knowledge, this is the first toolkit to support six Chinese NLP fundamental tasks. &lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>HIT-SCIR at MRP 2020: Transition-based Parser and Iterative Inference Parser</title>
        <published>2020-09-01T11:00:06.142+00:00</published>
        <updated>2020-09-01T11:00:06.142+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="http://18.218.69.233/publications/hit-scir-at-mrp-2020-transition-based-parser-and-iterative-inference-parser/"/>
        <id>http://18.218.69.233/publications/hit-scir-at-mrp-2020-transition-based-parser-and-iterative-inference-parser/</id>
        
        <content type="html" xml:base="http://18.218.69.233/publications/hit-scir-at-mrp-2020-transition-based-parser-and-iterative-inference-parser/">&lt;p&gt;This paper describes our submission system (HIT-SCIR) for the CoNLL 2020 shared task: Cross-Framework and Cross-Lingual Meaning Representation Parsing. &lt;&#x2F;p&gt;
&lt;p&gt;The task includes five frameworks for graph-based meaning representations, i.e., UCCA, EDS, PTG, AMR, and DRG. &lt;&#x2F;p&gt;
&lt;p&gt;Our solution consists of two sub-systems: &lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;transition-based parser for Flavor (1) frameworks (UCCA, EDS, PTG)&lt;&#x2F;li&gt;
&lt;li&gt;iterative inference parser for Flavor (2) frameworks (DRG, AMR). &lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;In the final evaluation, our system is ranked 3rd among the seven team both in Cross-Framework Track and Cross-Lingual Track, with the macro-averaged MRP F1 score of 0.81&#x2F;0.69.&lt;&#x2F;p&gt;
</content>
        
    </entry>
</feed>
